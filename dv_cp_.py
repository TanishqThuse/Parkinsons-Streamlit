# -*- coding: utf-8 -*-
"""DV CP .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fp7zUoPtEj8K4jkU2eCMKcHbyOpEinvC

# DATA VISUALIZATION COURSE PROJECT 2024-25

###Year: **SY**
###Division: **CSE AI B**
###Batch: **2**
###Group: **8**

###**Members:**
###33  12310543  SHREY RUPNAVAR
###37  12310120  ATHARVA SALITRI
###60  12310237  TANISHQ THUSE
###61  12311279  TRIPTI MIRANI

**Problem Statement:**
**Parkinson's disease (PD)** is a neurodegenerative disorder that affects millions of people worldwide. Early and accurate diagnosis of Parkinson's disease is crucial for effective management and treatment. Traditional diagnostic methods often rely on clinical evaluations, which can be subjective and may not detect early-stage PD.

**Machine learning (ML)** offers a potential solution by analyzing patterns in vocal measurements to distinguish between healthy individuals and those with Parkinson's disease. The Oxford Parkinson's Disease Detection Dataset provides a range of vocal features that can be used to train various ML algorithms for this purpose.

**Dataset:** https://archive.ics.uci.edu/dataset/174/parkinsons

###**Attribute Information:**

###**Matrix column entries (attributes):**

**name** - ASCII subject name and recording number

**MDVP:Fo(Hz)** - Average vocal fundamental frequency

**MDVP:Fhi(Hz)** - Maximum vocal fundamental frequency

**MDVP:Flo(Hz)** - Minimum vocal fundamental frequency

**MDVP:Jitter(%), MDVP:Jitter(Abs), MDVP:RAP, MDVP:PPQ, Jitter:DDP** - Several measures of variation in fundamental frequency

**MDVP:Shimmer,MDVP:Shimmer(dB),Shimmer:APQ3,Shimmer:APQ5,MDVP:APQ,Shimmer:DDA** - Several measures of variation in amplitude

**NHR, HNR** - Two measures of the ratio of noise to tonal components in the voice

**status** - The health status of the subject (one) - Parkinson's, (zero) - healthy

**RPDE, D2** - Two nonlinear dynamical complexity measures

**DFA** - Signal fractal scaling exponent

**spread1,spread2,PPE** - Three nonlinear measures of fundamental frequency variation
"""

# Commented out IPython magic to ensure Python compatibility.
# Import necessary libraries

import pickle
from sklearn.model_selection import cross_val_score
from sklearn.metrics import roc_curve, auc
# from xgboost import XGBClassifier
from sklearn.ensemble import VotingClassifier
from sklearn.naive_bayes import BernoulliNB
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
from sklearn.decomposition import PCA
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import RandomOverSampler
import plotly as py
from plotly.offline import iplot
import warnings
import os
import sys
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
sns.set(style="whitegrid")
warnings.filterwarnings("ignore")

# pip install plotly


pd.set_option('display.max_rows', 200)
pd.set_option('display.max_columns', 25)
pd.set_option('display.width', 400)

df = pd.read_csv('parkinsons.data')

print(df)

df.shape

# Handling Null Values

df.isnull().sum()

df.dtypes

df['status'].value_counts()

df.info()

# Distribution of data


def distplot(col):
    sns.distplot(df[col])
    plt.show()


for i in list(df.columns)[1:]:
    distplot(i)

# Outlier


def boxplots(col):
    sns.boxplot(df[col])
    plt.show()


for i in list(df.columns)[1:]:
    boxplots(i)

# Finding co-relation

plt.figure(figsize=(20, 20))
corr = df[df.columns[1:]].corr()
sns.heatmap(corr, annot=True, cmap="rainbow")

x = df.drop(['name', 'status'], axis=1)
y = df['status']

x.head()

y.head()


# Balancing the data

ros = RandomOverSampler()
x_ros, y_ros = ros.fit_resample(x, y)
print(y.value_counts())
print("##############")
print(y_ros.value_counts())

# Feature scaling

scaler = MinMaxScaler((-1, 1))
x = scaler.fit_transform(x_ros)
y = y_ros

"""**Basic pre-processing DONE!!!**"""

x.shape

"""##**Applying PCA**"""

pca = PCA(.95)
x_pca = pca.fit_transform(x)
print(x_pca.shape)

pd.DataFrame(x_pca)

x_train, x_test, y_train, y_test = train_test_split(
    x_pca, y, test_size=0.2, random_state=1)

"""## **Model Building**

### **MODEL 1: Logistic Regression**
"""


list_met = []
list_accuracy = []

# Logistic Regression
classifier = LogisticRegression(C=0.4, max_iter=1000, solver="liblinear")
lr = classifier.fit(x_train, y_train)
# Prediction
y_pred = classifier.predict(x_test)
# Accuracy
accuracy_LR = accuracy_score(y_test, y_pred)
print(accuracy_LR)

"""### **MODEL 2: Decision Tree**"""

classifier2 = DecisionTreeClassifier(random_state=14)
dt = classifier2.fit(x_train, y_train)
# Prediction
y_pred2 = classifier2.predict(x_test)
# Accuracy
accuracy_DT = accuracy_score(y_test, y_pred2)
print(accuracy_DT)

"""### **MODEL 3: Random Forest**"""

classifier3 = RandomForestClassifier(random_state=14)
rfi = classifier3.fit(x_train, y_train)
# Prediction
y_pred3 = classifier3.predict(x_test)
# Accuracy
accuracy_RF = accuracy_score(y_test, y_pred3)
print(accuracy_RF)

"""**MODEL 3.1**"""

classifier4 = RandomForestClassifier(criterion='entropy', random_state=14)
rfe = classifier4.fit(x_train, y_train)
# Prediction
y_pred4 = classifier4.predict(x_test)
# Accuracy
accuracy_RFE = accuracy_score(y_test, y_pred4)
print(accuracy_RFE)

"""### **MODEL 4: Support Vector Machine**"""


model_svm = SVC()
SVM = model_svm.fit(x_train, y_train)
# Prediction
y_pred5 = model_svm.predict(x_test)
# Accuracy
accuracy_SVM = accuracy_score(y_test, y_pred5)
print(accuracy_SVM)

"""### **MODEL 5: KNN**"""

model_knn = KNeighborsClassifier()
KNN = model_knn.fit(x_train, y_train)
# Prediction
y_pred6 = model_knn.predict(x_test)
# Accuracy
accuracy_KNN = accuracy_score(y_test, y_pred6)
print(accuracy_KNN)

"""### **MODEL 6: Gaussian Naive Bayes**"""

gnb = GaussianNB()
gnb = gnb.fit(x_train, y_train)
# Prediction
y_pred7 = gnb.predict(x_test)
# Accuracy
accuracy_GNB = accuracy_score(y_test, y_pred7)
print(accuracy_GNB)

"""### **MODEL 7: Bernoulli Naive Bayes**"""

model = BernoulliNB()
bnb = model.fit(x_train, y_train)
# Prediction
pred_bnb = model.predict(x_test)
# Accuracy
accuracy_BNB = accuracy_score(y_test, pred_bnb)
print(accuracy_BNB)

"""**Combining all models using Voting Classifier**"""


# Initialize the Voting Classifier
evc = VotingClassifier(estimators=[('lr', lr), ('DT', dt), ('RFI', rfi), ('RFE', rfe), (
    'SVC', SVM), ('KNN', KNN), ('GNB', gnb), ('BNB', bnb)], voting='hard', flatten_transform=True)

# Fit the model
model_evc = evc.fit(x_train, y_train)

# Prediction
y_pred_evc = evc.predict(x_test)

# Accuracy
accuracy_EV = accuracy_score(y_test, y_pred_evc)
print(accuracy_EV)

list1 = ["Logistic Regression", "Decision Tree", "Random Forest", "Random Forest Entropy", "Support Vector Machine",
         "K-Nearest Neighbors", "Gaussian Naive Bayes", "Bernoulli Naive Bayes", "Voting Classifier"]
list2 = [accuracy_LR, accuracy_DT, accuracy_RF, accuracy_RFE,
         accuracy_SVM, accuracy_KNN, accuracy_GNB, accuracy_BNB, accuracy_EV]
list3 = [classifier, classifier2, classifier3,
         classifier4, model_svm, model_knn, gnb, model, evc]

# Create a color palette with the desired colors
colors = ['red', 'green', 'blue', 'orange',
          'purple', 'brown', 'pink', 'gray', 'olive']

df_accuracy = pd.DataFrame({'Method Used': list1, "Accuracy": list2})
print(df_accuracy)

# charts = sns.barplot(x = "Method Used", y = "Accuracy", data = df_accuracy)
charts = sns.barplot(x="Method Used", y="Accuracy",
                     data=df_accuracy, palette=colors[:len(list1)])
charts.set_xticklabels(charts.get_xticklabels(), rotation=90)

print()
print(charts)

# model_xgb = XGBClassifier()
# model_xgb.fit(x_train, y_train)
# y_pred_xgb = model_xgb.predict(x_test)
# accuracy_XGB = accuracy_score(y_test, y_pred_xgb)
# print(accuracy_XGB)

"""## Other Evaluation Metrices"""


# RandomForest Entropy
y_pred4_train = classifier4.predict(x_train)
y_pred4_test = classifier4.predict(x_test)

# KNN
pred_knn_train = model_knn.predict(x_train)
pred_knn_test = model_knn.predict(x_test)

print(confusion_matrix(y_train, y_pred4_train))
print("********************************"*5)
print(confusion_matrix(y_test, y_pred4_test))

print(confusion_matrix(y_train, pred_knn_train))
print("********************************"*5)
print(confusion_matrix(y_test, pred_knn_test))

print(classification_report(y_train, y_pred4_train))
print("********************************"*5)
print(classification_report(y_test, y_pred4_test))

print(classification_report(y_train, pred_knn_train))
print("********************************"*5)
print(classification_report(y_test, pred_knn_test))


"""## **ROC and AUC**"""


def plot_roc(rfe, x_test, y_test):
    probabilities = model.predict_proba(np.array(x_test))
    predictions = probabilities
    fpr, tpr, threshold = roc_curve(y_test, predictions[:, 1])
    roc_auc = auc(fpr, tpr)
    plt.title('Receiver Operating Characteristic')
    plt.plot(fpr, tpr, 'b', label='AUC = %0.2f' % roc_auc)
    plt.legend(loc='lower right')
    plt.plot([0, 1], [0, 1], 'r--')
    plt.xlim([0, 1])
    plt.ylim([0, 1])
    plt.ylabel('True Positive Rate')
    plt.xlabel('False Positive Rate')
    plt.show()


plot_roc(rfe, x_test, y_test)


# Assuming 'classifier3' is your trained model
model_filename = 'parkinson_model.sav'
pickle.dump(classifier3, open(model_filename, 'wb'))
